"""
[íŒŒì¼ ì—­í• ]
RAG ê²€ìƒ‰ ê²°ê³¼ì™€ Ollama(ë¡œì»¬ LLM)ë¥¼ í•˜ë‚˜ë¡œ ë¬¶ì–´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” 'ì„œë¹„ìŠ¤ ë ˆì´ì–´'ì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ì§€ì‹ ì¡°ê°ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ AIê°€ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

[ì‹¤í–‰ ë°©ë²•]
1. ë¡œì»¬ì— Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³  `llama3.1` ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
2. `RagService` í´ë˜ìŠ¤ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ì—¬ `ask(ì§ˆë¬¸)` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.

[ë™ì‘ ìˆœì„œ]
1. `MilvusRetriever`ë¥¼ í†µí•´ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì¹˜ê³¼ ì§€ì‹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
2. ê²€ìƒ‰ëœ ì§€ì‹ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²°í•©í•˜ì—¬ ì „ìš© í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
3. Ollama ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ì—¬ ìµœì¢… ë‹µë³€ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
from typing import List
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from src.denticheck_ai.pipelines.rag.retrieve import MilvusRetriever
from src.denticheck_ai.pipelines.llm import prompts

class RagService:
    """
    RAG ê²€ìƒ‰ ê²°ê³¼ì™€ Ollama(Llama 3.1)ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ì§€ì‹ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í†µí•© ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    ë¹„ìš© 0ì›ìœ¼ë¡œ ë¡œì»¬ì—ì„œ ì‘ë™í•˜ëŠ” ì§€ëŠ¥í˜• ì¹˜ê³¼ ìƒë‹´ ì—”ì§„ì…ë‹ˆë‹¤.
    """
    
    def __init__(self, model_name: str = "llama3.1:latest"):
        """
        ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ê²€ìƒ‰ê¸°(Milvus)ì™€ ìƒì„±ê¸°(Ollama)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        
        Args:
            model_name (str): ì‚¬ìš©í•  Ollama ëª¨ë¸ëª…. ê¸°ë³¸ê°’ì€ 'llama3.1'.
        """
        # 1. ë¬¸ì„œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.retriever = MilvusRetriever()
        
        # 2. ë¡œì»¬ LLM (Ollama) ì´ˆê¸°í™”
        # Ollama ì„œë²„ ì£¼ì†Œ ì„¤ì • (Docker ë° ë¡œì»¬ í™˜ê²½ ëŒ€ì‘)
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        
        # 0ì›ì— ë¬´ì œí•œìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œì»¬ ëª¨ë¸ì…ë‹ˆë‹¤.
        self.llm = ChatOllama(
            model=model_name,
            base_url=base_url,
            temperature=0.2, # ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
        )
        
        self.output_parser = StrOutputParser()

    def _get_chain(self, language: str = "ko"):
        """ì–¸ì–´ë³„ë¡œ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        system_prompt = f"""ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì¹˜ê³¼ ì˜ì‚¬ 'ë´í‹°ì²´í¬ ì ê²€ë´‡'ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ [ê²€ìƒ‰ëœ ì§€ì‹]ë§Œì„ ê·¼ê±°ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë§Œì•½ [ê²€ìƒ‰ëœ ì§€ì‹]ì— ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µì´ ì—†ë‹¤ë©´, ì•„ëŠ” ë²”ìœ„ ë‚´ì—ì„œ êµ¬ê°• ê±´ê°• ìƒì‹ìœ¼ë¡œ ë‹µë³€í•˜ë˜ ì „ë¬¸ì ì¸ ì§„ë£ŒëŠ” ì¹˜ê³¼ ë°©ë¬¸ì´ í•„ìš”í•¨ì„ ë°˜ë“œì‹œ ì•ˆë‚´í•˜ì„¸ìš”.

[ê²€ìƒ‰ëœ ì§€ì‹]
{{context}}

{prompts.get_common_rules(language=language)}"""

        # ì˜ì–´ì¼ ê²½ìš° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë²ˆì—­ë³¸ ì ìš©
        if language == "en":
            system_prompt = f"""You are a friendly and professional dentist 'DentiCheck Bot'.
Answer the user's question based ONLY on the [Retrieved Knowledge] provided below.
If there is no direct answer in the [Retrieved Knowledge], answer with general oral health knowledge but ALWAYS state that a dental visit is required for a professional diagnosis.

[Retrieved Knowledge]
{{context}}

{prompts.get_common_rules(language=language)}"""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{question}"),
        ])
        
        return prompt | self.llm | self.output_parser

    def ask(self, question: str, language: str = "ko") -> str:
        """
        ì§ˆë¬¸ì— ëŒ€í•´ RAGë¥¼ ê±°ì³ ìµœì¢… ë‹µë³€ì„ í•œêº¼ë²ˆì— ìƒì„±í•©ë‹ˆë‹¤.
        """
        # 1. ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
        contexts = self.retriever.retrieve_context(question, top_k=3)
        context_text = "\n\n".join(contexts)
        
        # 2. ì–¸ì–´ë³„ ì²´ì¸ íšë“ ë° ì‹¤í–‰
        chain = self._get_chain(language=language)
        response = chain.invoke({
            "context": context_text,
            "question": question
        })
        
        return response

    def stream_ask(self, question: str, language: str = "ko"):
        """
        ì§ˆë¬¸ì— ëŒ€í•´ RAG ê²°ê³¼ì™€ í•¨ê»˜ ë‹µë³€ì„ í•œ ê¸€ìì”© ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        # 1. ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
        contexts = self.retriever.retrieve_context(question, top_k=3)
        context_text = "\n\n".join(contexts)
        
        # 2. ì–¸ì–´ë³„ ì²´ì¸ íšë“ ë° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        chain = self._get_chain(language=language)
        print(f"ğŸ¤– Ollama({self.llm.model})ê°€ ë‹µë³€({language})ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\n")
        return chain.stream({
            "context": context_text,
            "question": question
        })

if __name__ == "__main__":
    # ê°„ë‹¨í•œ ì—°ë™ í…ŒìŠ¤íŠ¸
    service = RagService()
    test_question = "ì‚¬ë‘ë‹ˆ ë½‘ê³  ë‚˜ì„œ ìˆ  ë§ˆì…”ë„ ë¼?"
    answer = service.ask(test_question)
    
    print("\n" + "="*50)
    print(f"ì§ˆë¬¸: {test_question}")
    print(f"ë‹µë³€: {answer}")
    print("="*50)
